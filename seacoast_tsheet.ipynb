{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmclgroup/DBStudio/blob/master/seacoast_tsheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W5e--AS6o1pC",
      "metadata": {
        "id": "W5e--AS6o1pC"
      },
      "source": [
        "#Quick Time Books Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0nC4jbfODlia",
      "metadata": {
        "id": "0nC4jbfODlia"
      },
      "source": [
        "#set_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6sQgW3Q46kAd",
      "metadata": {
        "id": "6sQgW3Q46kAd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title _dict\n",
        "\n",
        "name ='_dict'\n",
        "\n",
        "# set the values\n",
        "_dict = {}\n",
        "reporting = dict()\n",
        "_dict['proj'] = 'tsheet'\n",
        "_dict['admin'] = f'admin_{_dict.get(\"proj\")}.gsheet'\n",
        "_dict['root_dir'] = f'/My Drive/github/seacoast_{_dict.get(\"proj\")}'\n",
        "_dict['na_utility'] = '/My Drive/github/na_utility/na_utility.py'\n",
        "_dict['dir_in'] = '/data'\n",
        "_dict['dir_out'] = '/reporting'\n",
        "_dict['db'] = f'{_dict.get(\"proj\")}.db'\n",
        "_dict['uuid_dte_format'] = '%m/%d/%Y %H:%M:%S'\n",
        "#for repeatable uuid when using a dte column in uuid_dte_format.st\n",
        "_dict['uuid_nm_space'] = '6ba7b810-9dad-11d1-80b4-00c04fd430c8'\n",
        "_dict['dte_ymd'] = '%Y-%m-%d'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a16pr6_S4eWO",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a16pr6_S4eWO",
        "outputId": "0448f847-56ff-4430-8139-bd070ea10b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/github/seacoast_tsheet\n"
          ]
        }
      ],
      "source": [
        "#@title load_drive\n",
        "# %%script echo skipping\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "name ='load_drive'\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "  var.mount = '/content/gdrive'\n",
        "  drive.mount(var.mount)\n",
        "\n",
        "  # Switch to the directory on the Google Drive that you want to use\n",
        "  import os\n",
        "\n",
        "  # Create drive_root if it doesn't exist\n",
        "  var.dir = _dict['root_dir'] = var.mount + _dict.get('root_dir')\n",
        "  os.makedirs(var.dir , exist_ok=True)\n",
        "\n",
        "  # create the dir_in folder if it doesn't exist\n",
        "  var.dir = _dict['dir_in'] = _dict.get('root_dir') + _dict.get('dir_in')\n",
        "  os.makedirs(var.dir, exist_ok=True)\n",
        "\n",
        "  # create the dir_out folder if it doesn't exist\n",
        "  var.dir  = _dict['dir_out'] = _dict.get('root_dir') + _dict.get('dir_out')\n",
        "  os.makedirs(var.dir , exist_ok=True)\n",
        "\n",
        "  # Change to the directory\n",
        "  var.dir = _dict.get('root_dir')\n",
        "  %cd $var.dir\n",
        "\n",
        "# lighthousekeeping\n",
        "_dict['db_conn'] = f\"/{_dict.get('dir_in')}/{_dict.get('db')}\"\n",
        "\n",
        "try:\n",
        "  del IN_COLAB\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "JhEMVlWuJ37C",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhEMVlWuJ37C",
        "outputId": "0415cea1-9778-4fd8-81a3-679efd5f1203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/github/na_utility\n",
            "/content/gdrive/My Drive/github/seacoast_tsheet\n"
          ]
        }
      ],
      "source": [
        "# #@title import_modules\n",
        "\n",
        "# name = 'import_modules'\n",
        "\n",
        "# dir_ = _dict.get('root_dir')\n",
        "# %cd $dir_\n",
        "\n",
        "# from google.colab import files\n",
        "# from sys import platform\n",
        "# import gspread as gs\n",
        "# import os\n",
        "# import io\n",
        "# import re\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt  ##for exparto\n",
        "# import sqlite3\n",
        "\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# import gspread\n",
        "# from google.auth import default\n",
        "# creds, _ = default()\n",
        "# gc = gspread.authorize(creds)\n",
        "\n",
        "# from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "# try:\n",
        "#   import esparto as es\n",
        "# except ImportError as e:\n",
        "#   !pip install esparto\n",
        "#   import esparto as es\n",
        "\n",
        "# # housekeeping\n",
        "# del platform\n",
        "\n",
        "\n",
        "#@title import_modules\n",
        "name = 'import_modules'\n",
        "\n",
        "# there is a better way but this works!\n",
        "dir_ = '/content/gdrive/My Drive/github/na_utility/na_utility.ipynb'  #+ (_dictet('na_utility'))\n",
        "file_ = dir_.split('/')[-1]\n",
        "dir_  = '/'.join(dir_.split('/')[0:-1])\n",
        "%cd $dir_\n",
        "%run $file_\n",
        "dir_ = _dict.get('root_dir')\n",
        "%cd $dir_\n",
        "\n",
        "# from pandas._libs.tslibs.parsing import parse_time_string\n",
        "name ='import_modules'\n",
        "\n",
        "# google modules\n",
        "from google.colab import files\n",
        "import gspread as gs\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "# file and directory modules\n",
        "import os\n",
        "import io\n",
        "from sys import platform\n",
        "\n",
        "# base py modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import re\n",
        "import uuid\n",
        "import hashlib\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# try:\n",
        "#   import arrow\n",
        "# except ImportError as e:\n",
        "#   !pip install arrow\n",
        "#   import arrow\n",
        "\n",
        "# try:\n",
        "#   import geopandas as gd\n",
        "# except ImportError as e:\n",
        "#   !pip install geopandas\n",
        "#   import geopandas as gd\n",
        "\n",
        "try:\n",
        "  import esparto as es\n",
        "except ImportError as e:\n",
        "  !pip install esparto\n",
        "  import esparto as es\n",
        "\n",
        "# NOTEBOOK SPECIFIC refactor away from this module\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from functools import reduce\n",
        "from datetime import datetime as dt\n",
        "# from pandas._libs.tslibs.parsing import parse_time_string\n",
        "\n",
        "# lighthousekeeping\n",
        "del dir_,file_,platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "md2G92aQtf2d",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md2G92aQtf2d",
        "outputId": "846aa984-be47-4776-8204-09054a1d8860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "#@title my_proc\n",
        "%%script echo skipping\n",
        "\n",
        "\"\"\"\n",
        "centralizes my_proc used frequently for loading in import later\n",
        "\"\"\"\n",
        "\n",
        "def array_match(df_,match_):\n",
        "  \"\"\"find match between two df\"\"\"\n",
        "  r_ = re.compile(match_)\n",
        "  df_ = list(filter(r_.match, df_))\n",
        "  return df_\n",
        "\n",
        "def dir_ls(_val):\n",
        "  \"\"\"return the files in a folder, input directory name\"\"\"\n",
        "  %cd $_val\n",
        "  try:\n",
        "    _val_ = os.listdir(_val)\n",
        "    r = re.compile('.*\\..*')\n",
        "    _val_ = list(filter(r.match, _val_))  # get the files\n",
        "  except:\n",
        "    _val_ = 'error'\n",
        "  return _val_\n",
        "\n",
        "def conv_type(df_,val_,type_='str'):\n",
        "  \"\"\"convert columns to dtype, input [df,column_label,type], valid type = num, int, float, dte, string-default)\"\"\"\n",
        "  if pd.api.types.is_list_like(val_):\n",
        "    pass\n",
        "  else:\n",
        "    val_ = split_to_list(val_)\n",
        "  for item_ in val_:\n",
        "    if type_ == 'num':\n",
        "      df_[item_] = pd.to_numeric(df_[item_], errors='coerce')\n",
        "    elif type_ == 'int':\n",
        "      df_[item_] = pd.to_numeric(df_[item_], errors='coerce').fillna(0).astype(int)\n",
        "    elif type_ == 'float':\n",
        "      df_[item_] = pd.to_numeric(df_[item_], errors='coerce').fillna(0).astype(float)\n",
        "    elif type_ == 'dte':\n",
        "      df_[item_] = pd.to_datetime(df_[item_], errors='coerce')\n",
        "    else:\n",
        "      df_[item_] = df_[item_].apply(str).str.strip()\n",
        "  return df_\n",
        "\n",
        "def file_list(name):\n",
        "  \"\"\"doc tbd\"\"\"\n",
        "  r = data['class_admin']\n",
        "  # r = data['class_admin'].df\n",
        "  r = re.compile(r.loc[r.term == name,'translate'].item())\n",
        "  df = _dict.get('file_in')\n",
        "  df = list(filter(r.match, df))\n",
        "  return df\n",
        "\n",
        "def read_gsheet2(file_in,sheet_='sheet1'):\n",
        "  \"\"\"read gsheet tabs, input=[file_name,sheet_name] sheet_name default is sheet1\"\"\"\n",
        "  if '.gsheet' in file_in:\n",
        "    file_in = file_in.split(\".\")[0]\n",
        "    ws = gc.open(file_in).worksheet(sheet_)\n",
        "  else:\n",
        "    ws = gc.open_by_url(file_in).worksheet(sheet_)\n",
        "  rows = ws.get_all_values()\n",
        "  df = pd.DataFrame.from_records(rows[1:],columns=rows[0])\n",
        "  # df['fn_'] = file_in\n",
        "  return df\n",
        "\n",
        "def read_gsheet(file_in,sheet_='sheet1'):\n",
        "  \"\"\"read gsheet tabs, duplicate???\"\"\"\n",
        "  file_in = file_in.split(\".\")[0]\n",
        "  ws = gc.open(file_in).worksheet(sheet_)\n",
        "  rows = ws.get_all_values()\n",
        "  df = pd.DataFrame.from_records(rows[1:],columns=rows[0])\n",
        "  return df\n",
        "\n",
        "def load_file(file_list): #swap names\n",
        "  \"\"\"load files by file type utility\"\"\"\n",
        "  try:\n",
        "    df = pd.DataFrame()\n",
        "    if len(file_list) != 1:\n",
        "      for item_ in file_list:\n",
        "        listy = load_file_type(item_)\n",
        "        listy['fn'] = item_\n",
        "        df = pd.concat([df, listy])\n",
        "    else:\n",
        "      item_ = ''.join(file_list)\n",
        "      df = load_file_type(item_)\n",
        "      df['fn'] = item_\n",
        "    return df\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def load_file_type(file_in):\n",
        "  \"\"\"load files by file type utility, called by load_file\"\"\"\n",
        "  try:\n",
        "    fn_type = file_in.split(\".\")[-1]\n",
        "    types = ['csv','geojson','kml','xlsx','xls','gsheet']  #when other types of files come up add them\n",
        "    if fn_type in types:\n",
        "      if fn_type == \"csv\":\n",
        "        df = pd.read_csv(file_in)\n",
        "        # df['fn_dte'] = get_fn_dte(file_in)  ####------------------------------------problems with tsheets\n",
        "        # df['line'] = df.index   ####------------------------------------problems with tsheets\n",
        "      elif \"geojson\" in fn_type:\n",
        "        df = gd.read_file(file_in, driver='JSON')\n",
        "      elif \"kml\" in fn_type:\n",
        "        df = gd.read_file(file_in, driver='KML')\n",
        "      elif 'xlsx' in fn_type:\n",
        "        file_in = _dict.get('dir_in') + '/' + file_in\n",
        "        df = pd.read_excel(file_in, index_col=None, header=0)\n",
        "      elif 'xls' in fn_type:\n",
        "        file_in = _dict.get('dir_in') + '/' + file_in\n",
        "        df = pd.read_excel(file_in, index_col=None, header=0)\n",
        "      elif 'gsheet' in fn_type:\n",
        "        file_in = file_in.split(\".\")[0]\n",
        "        ws = gc.open(file_in).sheet1\n",
        "        rows = ws.get_all_values()\n",
        "        df = pd.DataFrame.from_records(rows[1:],columns=rows[0])\n",
        "      else:\n",
        "        print(f'something went wrong with {input}')\n",
        "      return df\n",
        "  except:\n",
        "    print(f'something went wrong with {input}')\n",
        "\n",
        "#XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "def get_fn_dte(file_in):\n",
        "  pass\n",
        "  file_path = os.getcwd() + '/' + file_in\n",
        "  date_modified_timestamp = os.path.getmtime(file_path)\n",
        "  date_modified = datetime.fromtimestamp(date_modified_timestamp)\n",
        "  return date_modified.strftime('%Y-%m-%d')\n",
        "#XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "\n",
        "def fn_exists(fn):\n",
        "  \"\"\"verify if file (fn) exists and notify\"\"\"\n",
        "  import os.path\n",
        "  try:\n",
        "    if os.path.isfile(fn):\n",
        "        print (\"File exist\")\n",
        "    else:\n",
        "        print (\"File not exist\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def list_enum(listy):\n",
        "  \"\"\"list enum df or other list like items\"\"\"\n",
        "  try:\n",
        "    for index, value in enumerate(listy):\n",
        "        print(index, value)\n",
        "  except TypeError:\n",
        "    print('TypeError')\n",
        "  except AttributeError:\n",
        "    print('AttributeError')\n",
        "    listy = listy.columns.values.tolist()\n",
        "    for index, value in enumerate(listy):\n",
        "        print(index, value)\n",
        "\n",
        "def split_to_list(fn,type='',delim=','):\n",
        "  \"\"\"good utility use all the time debugging\"\"\"\n",
        "  try:\n",
        "    fn = str(fn)\n",
        "    fn = fn.split(delim)\n",
        "    if type == 'int':\n",
        "      fn = list(map(int, fn))\n",
        "    return fn\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def col_dte (df,format='%Y-%m-%d'):  #format all date columns\n",
        "  \"\"\"string to datetime, input[df,format if different then the default]\"\"\"\n",
        "  try:\n",
        "    for item in [col for col in df.columns if 'dte' in col]:\n",
        "      df[item] = pd.to_datetime(df[item], errors='coerce').dt.strftime(format)\n",
        "  except:\n",
        "    return None\n",
        "  return df\n",
        "\n",
        "def gm_break (var = 'continue (y/n)'):\n",
        "  \"\"\"set breakpoints for debugging\"\"\"\n",
        "  var = input(var)\n",
        "  if var == 'n': 1/0\n",
        "\n",
        "# def gm_punch(input_,loc_= 'tbd'):\n",
        "def gm_punch(input_):\n",
        "  \"\"\"log information about sections and carry punchlist items for later development\"\"\"\n",
        "  reporting['punch_list'] += f'|{name}: {input_}'\n",
        "  print(f\"gm_rpt({input})\")\n",
        "  gm_break('refactor later? ')\n",
        "\n",
        "# def gm_doc(input_,loc_= 'tbd'):\n",
        "def gm_doc(input_):\n",
        "  \"\"\"log information about sections and carry documentation items for later display\"\"\"\n",
        "  reporting['doc_list'] += f'|{name}: {input_}'\n",
        "  print(f\"gm_rpt({input},doc)\")\n",
        "  gm_break('refactor later? ')\n",
        "\n",
        "def gm_rpt(input_,loc_ = 'punch'):\n",
        "  \"\"\"log information into reporting input=string, idx\"\"\"\n",
        "  if loc_ == 'punch':\n",
        "    reporting['punch_list'] += f'|{name}: {input_}'\n",
        "  else:\n",
        "    reporting['doc_list'] += f'|{name}: {input_}'\n",
        "\n",
        "def gm_set_uuid(df_,col_ = 'id_'):\n",
        "  \"\"\"input dataframe, column_label\"\"\"\n",
        "  # make sure there is a uuid for every record\n",
        "  # df_[col_] = [str(uuid.uuid4()) if pd.isna(value) or value == '' or value == 'nan' or value == 0 or value == 'None' else value for value in df_[col_]]\n",
        "  df_[col_] = [str(uuid.uuid4()) if value in (None,'',0,'nan','None',float('NaN')) else value for value in df_[col_]]\n",
        "  return df_\n",
        "\n",
        "def replace_with_NAN(df):\n",
        "  \"\"\"replaces nan and specified values with empty string\"\"\"\n",
        "  to_replace = ['', 0, 'nan']\n",
        "  for col in df.columns:\n",
        "    df[col].replace(to_replace, np.NAN, inplace=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YAw2Ajn-ejGR",
      "metadata": {
        "id": "YAw2Ajn-ejGR"
      },
      "source": [
        "#express_or_local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Z2JL6NDIeSLM",
      "metadata": {
        "id": "Z2JL6NDIeSLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fb9ce6-d049-462d-fed8-b837346961c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step through (y): \n"
          ]
        }
      ],
      "source": [
        "#title express_or_local\n",
        "# %%script echo skipping\n",
        "_dict['debug'] = input('step through (y): ')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4jIRM36YggG3",
      "metadata": {
        "id": "4jIRM36YggG3"
      },
      "source": [
        "\n",
        "# data_get"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title admin\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "name = 'admin'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "if _dict.get('debug') == 'y':\n",
        "  gm_break('admin break? :')\n",
        "\n",
        "df.admin = read_gsheet(_dict.get('admin'),'admin')\n",
        "var.no_tilde = ~df.admin['type_'].str.contains(r'~', regex=True)\n",
        "df.admin =  df.admin[var.no_tilde]\n",
        "_dict['admin_df'] = df.admin\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4rWyrpny8KuL"
      },
      "execution_count": 6,
      "outputs": [],
      "id": "4rWyrpny8KuL"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "name = 'load'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "_dict['file_in'] = os.listdir(_dict.get('dir_in'))\n",
        "var.admin = _dict.get('admin_df')\n",
        "var.load_fn = var.admin.loc[var.admin.type_.str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "var.load_fn.columns = split_to_list('_name,_table,_fn,_write')\n",
        "var.load_result = pd.DataFrame()\n",
        "\n",
        "def check_match(pattern, file_in_list):\n",
        "  return next((item for item in file_in_list if re.match(pattern, item)), None)\n",
        "\n",
        "for index, row in var.load_fn.iterrows():\n",
        "  var.temp = row\n",
        "  if 'http' not in row['_fn']:\n",
        "    var.temp._fn = check_match(row['_fn'],_dict.get('file_in'))\n",
        "  var.temp = pd.DataFrame(var.temp).transpose().copy()\n",
        "  var.load_result = pd.concat([var.load_result, var.temp])\n",
        "\n",
        "# filter out records that are marked with ~\n",
        "var.fn_missing = var.load_result.loc[var.load_result._fn.isnull(), '_table'].tolist()\n",
        "var.load_result = var.load_result.loc[var.load_result._fn.notnull()]\n",
        "var.admin_new = var.admin.loc[~var.admin.term.isin(var.fn_missing)]\n",
        "_dict['admin_df'] = var.admin_new.loc[~var.admin.translate.isin(var.fn_missing)]\n",
        "\n",
        "# # build class Data\n",
        "data = {}\n",
        "# data = {i: DataFile(i) for i in var.load_result._table}\n",
        "\n",
        "for index,row in var.load_result.iterrows():\n",
        "\n",
        "  if _dict.get('debug') == 'y':\n",
        "    gm_break(f\"{row['_table']}\") #<======================================BREAK\n",
        "\n",
        "  if 'gsheet' in row['_fn']:\n",
        "    df.temp = read_gsheet(row['_fn'],row['_table'])\n",
        "  elif 'https://docs.google.com/spreadsheets' in row['_fn']:\n",
        "    # this is very funky careful with it\n",
        "    df.temp = read_gsheet(row['_fn'],row['_write'])\n",
        "    row['_write'] = row['_table']\n",
        "  elif '.xls' in row['_fn']:\n",
        "    var.root = _dict.get('dir_in') + '/' + row['_fn']\n",
        "    df.temp = load_type_xls(var.root)\n",
        "  elif '.db' in row['_fn']:\n",
        "    var.root = _dict.get('dir_in') + '/' + row['_fn']\n",
        "    sql_in = DataSQL(var.root)\n",
        "    df.temp = sql_in.load_dataframe(row['_table'])\n",
        "    sql_in.close_connection()\n",
        "  elif '.geojson' in row['_fn']:\n",
        "    var.root = _dict.get('dir_in') + '/' + row['_fn']\n",
        "    df.temp = load_type_geojson(var.root)\n",
        "  elif '.csv' in row['_fn']:\n",
        "    var.root = _dict.get('dir_in') + '/' + row['_fn']\n",
        "    df.temp = load_type_csv(var.root)\n",
        "  else:\n",
        "    gm_break(f'not sure general case works {row[\"_table\"]} | {row[\"_fn\"]}')\n",
        "    df.temp = load_file(row['_fn'])\n",
        "\n",
        "  if row['_write'] not in data.keys():\n",
        "    print(f\"new key being created {row['_write']}\")\n",
        "\n",
        "  data[row['_write']] = df.temp\n",
        "\n",
        "# I need this later for data['hour_lake'] merge\n",
        "_dict['file_in'] = var.load_result\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21af93c1-b616-4123-8ac6-5affd7a75a8e",
        "cellView": "form",
        "id": "QQpGgT3ycAW4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new key being created admin\n",
            "new key being created hours\n",
            "new key being created hours_lake\n",
            "new key being created logs\n",
            "new key being created normalize\n",
            "new key being created pds\n",
            "new key being created pds_lake\n",
            "new key being created users\n"
          ]
        }
      ],
      "id": "QQpGgT3ycAW4"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "zBg9MQS1MVQc",
      "metadata": {
        "cellView": "form",
        "id": "zBg9MQS1MVQc"
      },
      "outputs": [],
      "source": [
        "#@title normalize\n",
        "# %%script echo skipping\n",
        "\n",
        "name = 'normalize'\n",
        "# if 'y' in _dict.get('debug','').lower(): gm_break()\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "var.norm = data[name]\n",
        "# var.norm = data[name].df\n",
        "var.admin = _dict.get('admin_df')\n",
        "var.admin = var.admin.loc[var.admin.type_ == name].reset_index(drop=True)\n",
        "var.admin.columns = split_to_list('del,_data,file_name,_table')\n",
        "\n",
        "for index, row in var.admin.iterrows():\n",
        "\n",
        "  if _dict.get('debug') == 'y':\n",
        "    gm_break(f\"{row['_data']}\") #<======================================BREAK\n",
        "\n",
        "  # if data[row['_data']].empty:\n",
        "  if row['_data'] in (data.keys()):\n",
        "    df.target = data[row['_data']]\n",
        "    # df.target = data[row['_data']].df\n",
        "    df.target.columns = [to_snake_case(col) for col in df.target.columns]\n",
        "\n",
        "    if len(df.target) == 0:\n",
        "      df.target = pd.DataFrame()\n",
        "\n",
        "    df.norm_target = var.norm.loc[var.norm.in_ == row['_data']].reset_index(drop=True)\n",
        "    df.target = df.target.rename(columns=dict(zip(df.norm_target.col_raw,df.norm_target.col_fin)))\n",
        "\n",
        "    # delete the unnecessary column labels\n",
        "    df.norm_target = df.norm_target.loc[df.norm_target.col_fin != 'delete']\n",
        "    if 'delete' in df.target:\n",
        "      df.target = df.target.drop(columns=['delete'])\n",
        "\n",
        "    # skinny if too much\n",
        "    if len(df.target.columns) > len(list(df.norm_target.col_fin)):\n",
        "      df.target = df.target.loc[:list(df.norm_target.col_fin)]\n",
        "    elif len(df.target.columns) < len(list(df.norm_target.col_fin)): # add if not enough\n",
        "      var.col_list = [value for value in df.norm_target.col_fin if value not in df.target.columns]\n",
        "      df.target[var.col_list] = pd.DataFrame(columns=var.col_list)\n",
        "\n",
        "    # columns in the right order and remove \"delete\"\n",
        "    df.norm_target = df.norm_target.sort_values(by='ord_fin')\n",
        "    df.target = df.target.reindex(columns=list(df.norm_target.col_fin))\n",
        "    df.target = df.target.loc[:,~df.target.columns.astype(str).str.contains('delete')].reset_index(drop=True).copy()\n",
        "\n",
        "    # right dtype for columns\n",
        "    df.target = conv_type(df.target,df.target.columns)\n",
        "    for var.col_type in list(df.norm_target.type_.unique()):\n",
        "      var.col_list = list(df.norm_target.loc[df.norm_target.type_ == var.col_type,'col_fin'])\n",
        "      df.target = conv_type(df.target,var.col_list,var.col_type)\n",
        "\n",
        "    df.target.fillna('', inplace=True)  # when convert to int empty assigned nan, this converts back\n",
        "    data[row['_data']] = df.target\n",
        "\n",
        "    del var.col_list,var.col_type,df.norm_target,df.target # lighthousekeeping\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hji9N2p0pcsX",
      "metadata": {
        "id": "hji9N2p0pcsX"
      },
      "source": [
        "# data_process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "tfRjZ0O6HZwr",
      "metadata": {
        "id": "tfRjZ0O6HZwr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title users_process\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "name = 'users_process'\n",
        "\n",
        "var.data_nm = 'users'\n",
        "\n",
        "if var.data_nm in data.keys():\n",
        "\n",
        "  if 'y' in _dict.get('debug','').lower(): gm_break()\n",
        "\n",
        "  df.target = data[var.data_nm]\n",
        "  var.admin = _dict.get('admin_df')\n",
        "  var.process = var.admin.loc[var.admin.type_.str.contains(name,regex=True,na=False)].reset_index(drop=True)\n",
        "  var.col = split_to_list('control,field,find,replace')\n",
        "  var.process.columns = var.col\n",
        "\n",
        "  for index,row in var.process.loc[var.process.control.str.contains('.*replace', regex=True, na=False)].iterrows():\n",
        "    df.target[row['field']] = df.target[row['field']].str.replace(row['find'],row['replace'], regex=True)\n",
        "  for index,row in var.process.loc[var.process.control.str.contains('.*extract', regex=True, na=False)].iterrows():\n",
        "    df.target[row['field']] = df.target[row['field']].str.extract(row['find'],expand = True)\n",
        "\n",
        "  # combine first and last name\n",
        "  df.target.loc[:, 'nm'] = df.target[['ln', 'nm']].T.agg(', '.join)\n",
        "  df.target = df.target.drop(['ln'], axis=1)\n",
        "\n",
        "  # process some cleanup\n",
        "  df.target.replace('nan', '', inplace=True)\n",
        "\n",
        "  # only one record per day so that idx will make sense\n",
        "  var.groupby = split_to_list('uid,dte_hired')\n",
        "  df.target['idx'] = df.target[var.groupby].astype(str).agg(''.join, axis=1)\n",
        "  df.target['idx'] = df.target['idx'].apply(lambda x: str(uuid.UUID(hashlib.md5(x.encode()).hexdigest())))\n",
        "\n",
        "  data[var.data_nm] = df.target\n",
        "\n",
        "  # create_map\n",
        "  # _dict['mask_name'] = df.target.loc[df.target['enabled'] == 'Y',['nm','unm','uid']]\n",
        "\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "LANLLVoyKqH0",
      "metadata": {
        "id": "LANLLVoyKqH0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title logs_process\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'logs_process'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "var.data_nm = 'logs'\n",
        "\n",
        "if var.data_nm in data.keys():\n",
        "\n",
        "  if 'y' in _dict.get('debug','').lower(): gm_break()\n",
        "\n",
        "  df.target = data[var.data_nm]\n",
        "  var.admin = _dict.get('admin_df')\n",
        "  var.no_tilde = ~var.admin['type_'].str.contains(r'~', regex=True)\n",
        "  var.admin =  var.admin[var.no_tilde]\n",
        "  var.process = var.admin.loc[var.admin.type_.str.contains(name,regex=True,na=False)].reset_index(drop=True)\n",
        "  var.col = split_to_list('control,field,find,replace')\n",
        "  var.process.columns = var.col\n",
        "\n",
        "  for index,row in var.process.loc[var.process.control.str.contains('.*replace', regex=True, na=False)].iterrows():\n",
        "    df.target[row['field']] = df.target[row['field']].str.replace(row['find'],row['replace'], regex=True)\n",
        "  for index,row in var.process.loc[var.process.control.str.contains('.*extract', regex=True, na=False)].iterrows():\n",
        "    df.target[row['field']] = df.target[row['field']].str.extract(row['find'],expand = True)\n",
        "\n",
        "  # do stuff\n",
        "  df.target = df.target.loc[df.target['unm_super'] != 'delete']\n",
        "  df.target = df.target.loc[~df.target['type'].str.contains('Locked timesheet', regex= True, na=False)]\n",
        "  df.target['super'] = ''\n",
        "  df.target['staff'] = ''\n",
        "  df.target.loc[df.target['unm_super'] == 'shop','super'] = 'shop'\n",
        "\n",
        "  # cross with users\n",
        "  df.append = data['users']\n",
        "  df.append = df.append.loc[df.append['enabled'] == 'Y'] # not sure I should skinny here?\n",
        "  # gm_break('')\n",
        "  # df.append = df.append.loc[df.append['active'] == 'active'] ### HOOK tsheets is handling archived diff\n",
        "  var.mask = dict(zip(df.append['uid'], df.append['nm']))\n",
        "  df.target.loc[df.target['super'] == '','super'] = df.target['uid_super'].map(var.mask)\n",
        "  df.target.loc[df.target['staff'] == '','staff'] = df.target['uid_staff'].map(var.mask)\n",
        "\n",
        "  # write back to logs\n",
        "  data[var.data_nm] = df.target.copy() #issue not carrying admin records for admin_fix report\n",
        "\n",
        "  # sum logs_build super and admin tables\n",
        "  var.col = split_to_list('unm_staff,staff,super,count')\n",
        "  df.target = df.target.groupby(var.col[0:-1]).agg({'staff':['count']}).reset_index()\n",
        "  df.target.columns = var.col\n",
        "  df.target = df.target.sort_values(by='count', ascending=True).drop_duplicates(subset='staff', keep='last').drop('count',axis=1)\n",
        "  _dict['supers'] = df.target.reset_index(drop=True)\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cfb9pg0IkTBb",
      "metadata": {
        "id": "cfb9pg0IkTBb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title hours_process\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "name = 'hours_process'\n",
        "var.data_nm = 'hours'\n",
        "\n",
        "if var.data_nm in data.keys():\n",
        "\n",
        "  if 'y' in _dict.get('debug','').lower(): gm_break()\n",
        "\n",
        "  df.target = data[var.data_nm]\n",
        "  var.admin = _dict.get('admin_df')\n",
        "  var.no_tilde = ~var.admin['type_'].str.contains(r'~', regex=True)\n",
        "  var.admin =  var.admin.loc[var.no_tilde]\n",
        "  var.process = var.admin.loc[var.admin.type_.str.contains(name,regex=True,na=False)].reset_index(drop=True)\n",
        "  var.col = split_to_list('control,field,find,replace')\n",
        "  var.process.columns = var.col\n",
        "\n",
        "  for index,row in var.process.loc[var.process.control.str.contains('.*replace', regex=True, na=False)].iterrows():\n",
        "    df.target[row['field']] = df.target[row['field']].str.replace(row['find'],row['replace'], regex=True)\n",
        "  for index,row in var.process.loc[var.process.control.str.contains('.*extract', regex=True, na=False)].iterrows():\n",
        "    df.target[row['field']] = df.target[row['field']].str.extract(row['find'],expand = True)\n",
        "\n",
        "  # get reporting file name now\n",
        "  var.min = df.target['dte_loc'].min().strftime('%Y-%m-%d')\n",
        "  var.max = df.target['dte_loc'].max().strftime('%Y-%m-%d')\n",
        "  _dict['file_name'] = f\"QB_hours_{var.min}_thru_{var.max}.html\"\n",
        "  reporting['intro'] = f\"QB_hours_{var.min}_thru_{var.max}\"\n",
        "\n",
        "  # add names back to hours for staff\n",
        "  df.target.loc[:, 'nm'] = df.target[['ln', 'nm']].T.agg(', '.join)\n",
        "  df.target = df.target.drop(['ln'], axis=1)\n",
        "\n",
        "  # report class missing\n",
        "  df.error = df.target.loc[df.target['class'].str.contains('nan')]\n",
        "  reporting['error_missing_class'] = df.error.loc[~df.error['job'].str.contains('Lunch|Vaca|Sick')]\n",
        "\n",
        "  #and super\n",
        "  df.target.replace('nan', '', inplace=True)\n",
        "  df.target = df.target.loc[~df.target['job'].str.contains('Lunch', regex=True, na=False)].copy()\n",
        "  df.supers = _dict.get('supers').sort_values(by='super')\n",
        "  var.mask = dict(zip(df.supers['unm_staff'], df.supers['super']))\n",
        "  df.target['super'] = df.target['unm_staff'].map(var.mask)\n",
        "\n",
        "  # clean up jobs\n",
        "  df.target['jc2'] = np.where( (df.target.jc2.isnull()),df.target.job,df.target.jc2)\n",
        "  df.target['class'] = np.where( (df.target['class'] == ''),df.target.group,df.target['class'])\n",
        "  df.target['job'] = df.target[['jc2','jc3']].apply(lambda x: ' '.join(x.dropna()), axis=1).str.rstrip()\n",
        "  df.target = df.target.drop(columns=['jc2','jc3']).copy()\n",
        "\n",
        "  # clean up dates\n",
        "  df.target['dte_loc'] = pd.to_datetime(df.target['dte_loc']).dt.strftime('%Y-%m-%d (%a)')\n",
        "\n",
        "  # create hook for sql_lake updates, only clean method\n",
        "  var.string = _dict.get('file_in')\n",
        "  var.string = var.string.loc[var.string['_table'] == var.data_nm,'_fn'].squeeze()\n",
        "  var.regex = r\"\\d{4}-\\d{2}-\\d{2}_thru_\\d{4}-\\d{2}-\\d{2}\"\n",
        "  var.string = re.search(var.regex, var.string)\n",
        "  if var.string:\n",
        "    var.string = var.string.group(0)\n",
        "  else:\n",
        "    gm_break('error condition creating file_name column')\n",
        "  df.target['group_idx'] = var.string\n",
        "\n",
        "  # write back to hours\n",
        "  data[var.data_nm] = df.target\n",
        "\n",
        "  ###################################\n",
        "  # gm_break('super all fudged up')\n",
        "  # find the last date worked and write it back to super\n",
        "  df.target = df.target.sort_values(by='dte_loc', ascending=True).drop_duplicates('unm_staff',keep='last')\n",
        "  var.mask = dict(zip(df.target['unm_staff'], df.target['dte_loc']))\n",
        "  df.supers['last_day'] = df.supers['unm_staff'].map(var.mask)\n",
        "  _dict['supers'] = df.supers\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "XieGC2LY2a3f",
      "metadata": {
        "cellView": "form",
        "id": "XieGC2LY2a3f"
      },
      "outputs": [],
      "source": [
        "#@title pds_process\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'pds_process'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "var.data_nm = 'pds'\n",
        "\n",
        "if var.data_nm in data.keys():\n",
        "\n",
        "  if 'y' in _dict.get('debug','').lower(): gm_break()\n",
        "\n",
        "  df.target = data[var.data_nm]\n",
        "  var.admin = _dict.get('admin_df')\n",
        "  var.no_tilde = ~var.admin['type_'].str.contains(r'~', regex=True).copy()\n",
        "  var.admin =  var.admin[var.no_tilde]\n",
        "  var.process = var.admin.loc[var.admin.type_.str.contains(name,regex=True,na=False)].reset_index(drop=True).copy()\n",
        "  var.col = split_to_list('control,field,find,replace')\n",
        "  var.process.columns = var.col\n",
        "\n",
        "  for index,row in var.process.loc[var.process.control.str.contains('.*replace', regex=True, na=False)].iterrows():\n",
        "    df.target[row['field']] = df.target[row['field']].str.replace(row['find'],row['replace'], regex=True)\n",
        "  for index,row in var.process.loc[var.process.control.str.contains('.*extract', regex=True, na=False)].iterrows():\n",
        "    df.target[row['field']] = df.target[row['field']].str.extract(row['find'],expand = True)\n",
        "\n",
        "  # update pds with information from users\n",
        "  var.col_fin = df.target.columns.tolist()\n",
        "  df.target.set_index('unm_staff', inplace=True)\n",
        "  df.append = data['users'].set_index('unm').drop(columns='super')\n",
        "\n",
        "  # add what is brand new\n",
        "  var.mask = pd.merge(df.target, df.append, left_index=True, right_index=True, how='outer', indicator=True)\n",
        "  var.mask = var.mask.loc[var.mask['_merge']=='right_only'] #.index.to_list()\n",
        "  if not var.mask.empty:\n",
        "    var.mask = var.mask.index.to_list()\n",
        "    df.append_new = df.append.loc[df.append.index.isin(var.mask)]\n",
        "    df.target = pd.concat([df.target,df.append_new], ignore_index=False)\n",
        "\n",
        "  # update all but super and last worked\n",
        "  var.col = split_to_list('em,group,enabled,active') # only update what changes\n",
        "  # gm_break('only update active/enabled')\n",
        "  df.target.update(df.append[var.col])\n",
        "\n",
        "  # calculate vacation dates\n",
        "  df.target['vac_1'] = pd.to_datetime(df.target['dte_hired'].apply(lambda x: x.split(' ')[0]), errors='coerce') + pd.DateOffset(years=2)\n",
        "  df.target['vac_2'] = df.target['vac_1'] + pd.DateOffset(years=3)\n",
        "  df.target['vac_1'] = df.target['vac_1'].dt.strftime('%Y-%m-%d')\n",
        "  df.target['vac_2'] = df.target['vac_2'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "  # add back super and last_day\n",
        "  df.append = _dict.get('supers')\n",
        "  var.mask = dict(zip(df.append['unm_staff'], df.append['super']))\n",
        "  df.target['super'] = df.target.index.map(lambda x: var.mask[x] if x in var.mask else df.target['super'][x])\n",
        "  var.mask = dict(zip(df.append['unm_staff'], df.append['last_day']))\n",
        "  df.target['last_day'] = df.target.index.map(lambda x: var.mask[x] if x in var.mask else df.target['last_day'][x])\n",
        "  # gm_break('breakpoint')\n",
        "\n",
        "  # get the total paid vacation taken\n",
        "  df.append = data['hours_lake']\n",
        "  df.append = df.append.loc[df.append['job'] == 'Vacation (paid)']\n",
        "  var.col = split_to_list('unm_staff,dte_loc')\n",
        "  df.append = df.append.groupby(var.col[0:-1]).agg({var.col[-1]: ['count']}).reset_index()\n",
        "  df.append.columns = split_to_list('unm_staff,vac_taken')\n",
        "  df.append.set_index('unm_staff', inplace=True)\n",
        "  df.target.update(df.append['vac_taken'])\n",
        "\n",
        "  # write back to pds\n",
        "  df.target = df.target.reset_index(drop=False)\n",
        "  data[var.data_nm] = df.target[var.col_fin]\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data_write"
      ],
      "metadata": {
        "id": "6kbIsr-VabMO"
      },
      "id": "6kbIsr-VabMO"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "8b1Dj4DTabMP"
      },
      "outputs": [],
      "source": [
        "#@title data_merge\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'data_merge'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "var.admin = _dict.get('admin_df')\n",
        "var.no_tilde = ~var.admin['type_'].str.contains(r'~', regex=True)\n",
        "var.admin =  var.admin[var.no_tilde]\n",
        "var.process = var.admin.loc[var.admin['type_'].str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "var.process.columns = split_to_list('del,_target,_append,_write')\n",
        "\n",
        "for index, row in var.process.iterrows():\n",
        "\n",
        "  if 'y' in _dict.get('debug','').lower():\n",
        "    gm_break(f\"{row['_target']} |update-append-delete| {row['_append']}\") #<================================================BREAK\n",
        "\n",
        "  # Get the target\n",
        "  var.target = str.split(row['_target'],'||')\n",
        "  var.target, var.target_idx = var.target\n",
        "  if var.target in (data.keys()):\n",
        "    df.target = data.get(var.target)\n",
        "    df.target = df.target.loc[df.target[var.target_idx] != '']\n",
        "  else: df.target = None\n",
        "\n",
        "  # Get the append\n",
        "  var.append = str.split(row['_append'],'||')\n",
        "  var.append, var.append_idx = var.append\n",
        "  if var.append in (data.keys()):\n",
        "    df.append = data[var.append]\n",
        "    df.append = df.append.loc[df.append[var.append_idx] != '']\n",
        "  else: df.append = None\n",
        "\n",
        "  # Check if either DataFrame is None\n",
        "  if df.target is None or df.append is None:\n",
        "    gm_break(\"One or both DataFrames do not exist.\")\n",
        "\n",
        "  # make sure columns match or create the union of columns\n",
        "  if set(df.target.columns) != set(df.append.columns):\n",
        "    gm_break('col procedure untested')\n",
        "    var.cols = df.target.columns.union(df.append.columns)\n",
        "    df.target = df.target.reindex(columns=var.cols)\n",
        "    df.append = df.append.reindex(columns=var.cols)\n",
        "  else: var.cols = df.target.copy().columns.to_list()\n",
        "\n",
        "  # merge the two dataframes update matches and add missing\n",
        "  df.target = df.target.drop_duplicates(subset=[var.target_idx], keep='last')\n",
        "  df.append = df.append.drop_duplicates(subset=[var.append_idx], keep='last')\n",
        "\n",
        "  var.mask = pd.merge(df.target[var.target_idx], df.append[var.append_idx], on=var.target_idx, how='outer', indicator=True, suffixes=('', ''))\n",
        "  df.target.set_index(var.target_idx,drop=True, inplace=True)\n",
        "  df.append.set_index(var.append_idx,drop=True, inplace=True)\n",
        "\n",
        "  if var.mask['_merge'].isin(['both']).any():  # update matching\n",
        "    df.target.update(df.append)\n",
        "\n",
        "  if var.mask['_merge'].isin(['right_only']).any(): # add new\n",
        "    var.mask = var.mask.loc[var.mask['_merge'] == 'right_only']\n",
        "    df.append = df.append.loc[df.append.index.isin(var.mask[var.append_idx])]\n",
        "    df.target = pd.concat([df.target,df.append], ignore_index=False)\n",
        "\n",
        "  # write to the upload file\n",
        "  df.target.reset_index(drop=False, inplace=True)\n",
        "  data[row['_write']] = df.target[var.cols]\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  sql_in.close_connection()\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ],
      "id": "8b1Dj4DTabMP"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title data_group_replace\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'data_group_replace'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "var.admin = _dict.get('admin_df')\n",
        "var.no_tilde = ~var.admin['type_'].str.contains(r'~', regex=True)\n",
        "var.admin =  var.admin[var.no_tilde]\n",
        "var.process = var.admin.loc[var.admin['type_'].str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "var.process.columns = split_to_list('del,_target,_append,_write')\n",
        "\n",
        "for index, row in var.process.iterrows():\n",
        "\n",
        "  if 'y' in _dict.get('debug','').lower():\n",
        "    gm_break(f\"{row['_target']} |update-append-delete| {row['_append']}\") #<================================================BREAK\n",
        "\n",
        "  # Get the target\n",
        "  var.target = str.split(row['_target'],'||')\n",
        "  var.target, var.target_idx = var.target\n",
        "  if var.target in (data.keys()):\n",
        "    df.target = data.get(var.target)\n",
        "    df.target = df.target.loc[df.target[var.target_idx] != '']\n",
        "  else: df.target = None\n",
        "\n",
        "  # Get the append\n",
        "  var.append = str.split(row['_append'],'||')\n",
        "  var.append, var.append_idx = var.append\n",
        "  if var.append in (data.keys()):\n",
        "    df.append = data[var.append]\n",
        "    df.append = df.append.loc[df.append[var.append_idx] != '']\n",
        "  else: df.append = None\n",
        "\n",
        "  # Check if either DataFrame is None\n",
        "  if df.target is None or df.append is None:\n",
        "    gm_break(\"One or both DataFrames do not exist.\")\n",
        "\n",
        "  # make sure columns match or create the union of columns\n",
        "  if set(df.target.columns) != set(df.append.columns):\n",
        "    gm_break('col procedure untested')\n",
        "    var.cols = df.target.columns.union(df.append.columns)\n",
        "    df.target = df.target.reindex(columns=var.cols)\n",
        "    df.append = df.append.reindex(columns=var.cols)\n",
        "  else: var.cols = df.target.copy().columns.to_list()\n",
        "\n",
        "  # remove the grouping from the target\n",
        "  var.idx_remove = df.append[var.append_idx].unique().tolist()[0]\n",
        "  df.target = df.target.loc[~df.target[var.target_idx].str.contains(var.idx_remove)]\n",
        "  df.append = df.append.loc[df.append[var.append_idx].str.contains(var.idx_remove)]\n",
        "\n",
        "  # merge the two dataframes\n",
        "  if not df.target.empty:\n",
        "    df.target = pd.concat([df.target, df.append]).reset_index(drop=True)\n",
        "  else:\n",
        "    df.target = df.append.copy()\n",
        "\n",
        "  # write to the upload file\n",
        "  data[row['_write']] = df.target[var.cols]\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  sql_in.close_connection()\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ],
      "metadata": {
        "cellView": "form",
        "id": "l82TtOZepqGY"
      },
      "id": "l82TtOZepqGY",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "id": "rXFixETfRG_1"
      },
      "outputs": [],
      "source": [
        "#@title data_sql\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'data_sql'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "# var.db = f\"/{_dict.get('dir_in')}/{_dict.get('db')}\"\n",
        "sql_in = DataSQL(_dict.get('db_conn'))\n",
        "\n",
        "var.admin = _dict.get('admin_df')\n",
        "var.no_tilde = ~var.admin['type_'].str.contains(r'~', regex=True)\n",
        "var.admin =  var.admin[var.no_tilde]\n",
        "var.process = var.admin.loc[var.admin['type_'].str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "var.process.columns = split_to_list('del,_target,_append,_write')\n",
        "\n",
        "for index, row in var.process.iterrows():\n",
        "\n",
        "  if 'y' in _dict.get('debug','').lower():\n",
        "    gm_break(f\"{row['_target']} |sql write| {row['_append']}\") #<================================================BREAK\n",
        "\n",
        "  # Get the target\n",
        "  # df.target = row['_target']\n",
        "  if row['_target'] in (data.keys()):\n",
        "    df.target = data.get(row['_target'])\n",
        "  else: df.target = None\n",
        "\n",
        "  sql_in.save_dataframe(df.target,row['_write'])\n",
        "\n",
        "sql_in.close_connection()\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  sql_in.close_connection()\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var,sql_in"
      ],
      "id": "rXFixETfRG_1"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "seOTuhjtpUm8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title gsheet_update\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'gsheet_update'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "var.admin = _dict.get('admin_df')\n",
        "var.no_tilde = ~var.admin['type_'].str.contains(r'~', regex=True)\n",
        "var.admin =  var.admin[var.no_tilde]\n",
        "var.process = var.admin.loc[var.admin['type_'].str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "var.process.columns = split_to_list('_func,_read,_fn,_write')\n",
        "\n",
        "for index, row in var.process.iterrows():\n",
        "\n",
        "  if 'y' in _dict.get('debug','').lower():\n",
        "    gm_break(f\"{row['_func']}|{row['_write']} \") #<================================================BREAK\n",
        "\n",
        "  if row['_read'] in (data.keys()):\n",
        "    df.target = data.get(row['_read'])\n",
        "\n",
        "    # if len(df.target) == 0:\n",
        "    #   df.target = pd.DataFrame()\n",
        "\n",
        "    df.target.fillna('', inplace=True)  # when convert to int empty assigned nan, this converts back\n",
        "\n",
        "    gsheets_write_df(row['_fn'],row['_write'],df.target)\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ],
      "id": "seOTuhjtpUm8"
    },
    {
      "cell_type": "markdown",
      "id": "G9lH3GlzfczQ",
      "metadata": {
        "id": "G9lH3GlzfczQ"
      },
      "source": [
        "# build_rpts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Qj9ODqYyrF93",
      "metadata": {
        "cellView": "form",
        "id": "Qj9ODqYyrF93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a877d66a-e59a-424c-912f-759e0267409d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "#@title rpt_admin_fixed\n",
        "%%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_admin_fixed'\n",
        "\n",
        "df0 = data['logs']\n",
        "c0 = _dict.get('admin_df')\n",
        "# db_sql.load_dataframe('class_admin')\n",
        "c0 = c0.loc[c0.type_.str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "\n",
        "# do something\n",
        "c1 = _dict.get('admin')\n",
        "df00 = df0.loc[~df0.super.isin(c1.nm)].reset_index(drop=True) # don't need admin in log_in after this\n",
        "df0 = df0.loc[df0.super.isin(c1.nm)].reset_index(drop=True)\n",
        "\n",
        "df1 = data['pds_in']\n",
        "df1 = df1.loc[df1.enabled == 'Y']\n",
        "df0['admin'] = df0['super']\n",
        "df0['super'] = df0['staff'].map(df1.set_index('nm')['super'])\n",
        "c1 = split_to_list(c0.loc[c0.term.str.contains('col', regex= True, na=False),'translate'].reset_index(drop=True)[0])\n",
        "df0 = df0.groupby(c1[0:-1]).agg({c1[-1]: ['count']}).reset_index()\n",
        "df0 = df0.set_axis(c1, axis=1).sort_values(by=c1)\n",
        "\n",
        "try:\n",
        "  df0 = df0.pivot_table(index=c1[0:-2], columns=c1[-2], values=c1[-1], aggfunc='sum', margins=True, margins_name='Total').fillna(value=\" \")#.reset_index()\n",
        "except:\n",
        "  df0 = \"nothing to report\"\n",
        "\n",
        "# report something\n",
        "reporting[name] = df0\n",
        "data['logs'] = df00\n",
        "\n",
        "#lighthousekeeping\n",
        "del c0,c1,df0,df1,df00"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a9227da9-c6d6-424e-89fd-f9ae840041e3",
      "metadata": {
        "cellView": "form",
        "id": "a9227da9-c6d6-424e-89fd-f9ae840041e3"
      },
      "outputs": [],
      "source": [
        "#@title rpt_multiple_super\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_multiple_super'\n",
        "\n",
        "df0 = data['logs']\n",
        "c0 = _dict.get('admin_df')\n",
        "\n",
        "# do stuff\n",
        "df0 = df0[df0.super != df0.staff].copy() # skinny to hours entered by someone else\n",
        "c1 = split_to_list('staff,super,count')\n",
        "df0 = df0.groupby(c1[0:-1]).agg({'staff': ['count']}).reset_index()\n",
        "df0.columns = c1 # flatten the column multi-index\n",
        "df0 = df0.sort_values(['staff','count'])\n",
        "df0['flag'] = df0.duplicated(keep=False, subset=['staff'])  # find two supers\n",
        "df0 = df0[df0.flag == True].sort_values(['staff','super'],ascending=True)\n",
        "\n",
        "# #report something\n",
        "if len(df0):\n",
        "  reporting[name] = df0.astype(str).groupby('staff').agg(lambda x: ' | '.join(x.unique())).drop(columns=['flag'])\n",
        "else:\n",
        "  reporting[name] = 'nothing to report'\n",
        "\n",
        "\n",
        "#lighthousekeeping\n",
        "del c0,c1,df0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c2ee9488-6c60-4439-8270-feb1622fc22d",
      "metadata": {
        "id": "c2ee9488-6c60-4439-8270-feb1622fc22d",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title rpt_no_hours\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_no_hours'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "var.admin = _dict.get('admin_df')\n",
        "var.no_tilde = ~var.admin['type_'].str.contains(r'~', regex=True)\n",
        "var.admin =  var.admin[var.no_tilde]\n",
        "var.process = var.admin.loc[var.admin.type_.str.contains(name,regex=True,na=False)].reset_index(drop=True)\n",
        "var.col = split_to_list('control,field,find,replace')\n",
        "var.process.columns = var.col\n",
        "\n",
        "df0 = data['users']\n",
        "df1 = data['hours']\n",
        "\n",
        "# clean up stuff\n",
        "df0 = df0.loc[df0.enabled == 'Y']  #enabled has disappeared\n",
        "df0 = df0[~df0.unm.isin(df1.unm_staff)] # hours missing\n",
        "df0 = df0.loc[~df0['unm'].isin(var.process['find'])]\n",
        "\n",
        "# df0 = df0[~df0.uid.isin(c0)]\n",
        "df1 = data['pds']\n",
        "df1 = df1.loc[df1['unm_staff'].isin(df0['unm'])]\n",
        "\n",
        "if len(df1):\n",
        "  var.col = split_to_list('group,super,nm,dte_hired,last_day')\n",
        "  df1 = df1[var.col].sort_values(by=['group','super','nm']).reset_index(drop=True)\n",
        "  reporting[name] = df1\n",
        "else:\n",
        "  reporting[name] = 'nothing to report'\n",
        "\n",
        "# lighthousekeeping\n",
        "try:\n",
        "  del index,row,df0,df1\n",
        "except:\n",
        "  pass\n",
        "finally:\n",
        "  del df,var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cb059f40-cc9b-4160-bc7c-63e0502aea82",
      "metadata": {
        "cellView": "form",
        "id": "cb059f40-cc9b-4160-bc7c-63e0502aea82"
      },
      "outputs": [],
      "source": [
        "#@title rpt_hours_double\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_hours_double'\n",
        "\n",
        "df0 = data['hours']\n",
        "c0 = _dict.get('admin_df')\n",
        "c0 = c0.loc[c0.type_.str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "\n",
        "#do something\n",
        "c1 = split_to_list('job,dte_loc,nm,hours')\n",
        "# c1 = split_to_list(c0.loc[c0.term.str.contains('col', regex= True, na=False),'translate'].reset_index(drop=True)[0])\n",
        "df0 = df0.loc[df0['class'] == 'Field'].copy() #default gets the field supers not shop\n",
        "df0 = df0.pivot_table(index=c1[0:-1], values=c1[-1], aggfunc={c1[-1]: ['count','sum']}) #.reset_index()\n",
        "df0 = df0.loc[df0['count'] > 1]\n",
        "\n",
        "#report something\n",
        "reporting[name] = df0\n",
        "\n",
        "#lighthousekeeping\n",
        "del c0,c1,df0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ce956c26-e536-4c36-bb33-777059086d64",
      "metadata": {
        "cellView": "form",
        "id": "ce956c26-e536-4c36-bb33-777059086d64"
      },
      "outputs": [],
      "source": [
        "#@title rpt_notes\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_notes'\n",
        "\n",
        "df0 = data['hours']\n",
        "c0 = _dict.get('admin_df')\n",
        "c0 = c0.loc[c0.type_.str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "\n",
        "#do something\n",
        "df0.replace('nan', '', inplace=True)\n",
        "df0 = df0.loc[(df0.note != '')].copy()\n",
        "c0 = split_to_list(c0.loc[c0.term.str.contains('col', regex= True, na=False),'translate'][0])\n",
        "df0 = df0[c0]\n",
        "df0 = df0.pivot_table(index=c0[0:-2], columns=c0[-2], values=c0[-1]).fillna(value=\" \")  #.reset_index()\n",
        "\n",
        "# #report something\n",
        "reporting[name] = df0\n",
        "\n",
        "#lighthousekeeping\n",
        "del c0,df0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "05863df6-b01e-4fe5-b4d4-ff696f940285",
      "metadata": {
        "cellView": "form",
        "id": "05863df6-b01e-4fe5-b4d4-ff696f940285"
      },
      "outputs": [],
      "source": [
        "#@title ~rpt_total_hours\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_total_hours'\n",
        "# gm_rpt('highligh count > 1, super > 1')\n",
        "\n",
        "df0 = data['hours']\n",
        "c0 = _dict.get('admin_df')\n",
        "c0 = c0.loc[c0.type_.str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "# gm_break()\n",
        "# do stuff\n",
        "df0.loc[df0.group == 'Shop', 'super'] = 'Shop'  # carve out the shop\n",
        "c1 = split_to_list('jasondillman@gmail.com')\n",
        "df0.loc[df0.unm_staff.isin(c1),'super'] = 'Shop'\n",
        "# c1 = split_to_list('serinity2011@gmail.com,23bintliffm@gmail.com,warren@seacoastinc.net,jbakerjohn9@gmail.com')\n",
        "# df0.loc[ df0.uid.isin(c1),'super'] = 'Mechanical'\n",
        "# c1 = split_to_list('gordo0519@yahoo.com,carlwalker703@gmail.com,alex.contreras1987.jerry@gmail.com,franklukas541185@gmail.com,isaccdecorah@gmail.com,jfarfsing,nikparianos@icloud.com,123nikolasp@gmail.com,pridgeonivory@gmail.com,johnv69sb@gmail.com')\n",
        "# df0.loc[ df0.uid.isin(c1),'super'] = 'Parianos, Nikolas'\n",
        "# c1 = split_to_list('jorgitolopeznarvaez@gmail.com,jnieves,jruiz,lrivera,marcossantiago406@gmail.com,adolfo.sierra26@gmail.com,aureliosierra4@gmail.com,ryounkin@verizon.net')\n",
        "# df0.loc[ df0.uid.isin(c1),'super'] = 'Sierra, Aurelio'\n",
        "\n",
        "# Normal reporting total hours\n",
        "c1 = split_to_list(\"class_,job,super,nm,dte_loc,hours\")\n",
        "df1 = df0[split_to_list('nm,job')]\n",
        "df1 = df1.astype(str).groupby('nm').agg(lambda x: ' | '.join(x.unique()))\n",
        "df0['work_at'] = df0['nm'].map(df1['job'])\n",
        "\n",
        "df1 = d0_ = df0 # for later report for lisa\n",
        "\n",
        "c1 = split_to_list(\"class,super,nm,work_at,dte_loc,hours\")\n",
        "df0 = df0.groupby(c1[0:-1]).agg({c1[-1]: ['sum']}).reset_index()\n",
        "df0 = df0.set_axis(c1, axis=1)  # refactor else where\n",
        "df0 = df0.pivot_table(index=c1[1:-2], columns=c1[-2], values=c1[-1], aggfunc='sum', margins=True, margins_name='Total').fillna(value=\" \")#.reset_index()\n",
        "\n",
        "# #lisa special report\n",
        "c1 = df1['nm'].str.split(',', expand=True)\n",
        "c1.columns = ['ln','fn_']\n",
        "df1 = pd.concat([df1, c1], axis=1)\n",
        "df1['nm'] = df1['fn_'] + ' ' + df1['ln']\n",
        "df1['entry'] = df1.job + ' | ' +  df1['class'] + ' | ' + df1.cost_code\n",
        "\n",
        "c1 = split_to_list('nm,entry,hours')\n",
        "df1 = df1.groupby(c1[0:-1]).agg({c1[-1]: ['sum']}).reset_index()\n",
        "reporting['Lisa_rpt'] = df1.set_axis(c1, axis=1)  # refactor else where\n",
        "# gm_break()\n",
        "# df1 = df1.set_index(c0[0:-1])\n",
        "# # df1 = df1.pivot_table(index=c1[0:-2], columns=c1[-2], values=c1[-1], aggfunc='sum', margins=True, margins_name='Total').fillna(value=\" \")#.reset_index()\n",
        "\n",
        "reporting[name] = df0\n",
        "\n",
        "#add highlight nm in multiple super\n",
        "#add str sum @jobs\n",
        "#add highlight, more than one record\n",
        "#add highlight, over or under?\n",
        "\n",
        "# housekeeping\n",
        "del c0,c1,df0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a9e333fc-4d37-488e-879a-18f55aab93e8",
      "metadata": {
        "id": "a9e333fc-4d37-488e-879a-18f55aab93e8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title rpt_job_hours\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_job_hours'\n",
        "\n",
        "df0 = data['hours']\n",
        "c0 = _dict.get('admin_df')\n",
        "c0 = c0.loc[c0.type_.str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "\n",
        "# print in sections\n",
        "for row in pd.DataFrame(df0['class'].unique()).itertuples(index=False):\n",
        "  # gm_break()\n",
        "  pass\n",
        "  # if row[0] == 'Field':gm_break(f'{row[0]}')\n",
        "  df1 = df0.loc[df0['class'] == row[0]]\n",
        "  c1 = c0.loc[c0.term.str.contains(row[0], regex= True, na=False),'translate'].reset_index(drop=True)[0]\n",
        "  c1 = split_to_list(c1)\n",
        "\n",
        "  df1 = df1.loc[:,c1]\n",
        "\n",
        "  df1 = df1.groupby(c1[0:-1]).agg({c1[-1]: ['sum']}).reset_index()\n",
        "  df1 = df1.set_axis(c1, axis=1)\n",
        "  df1 = df1.pivot_table(index=c1[1:-2], columns=c1[-2], values=c1[-1], aggfunc='sum', margins=True, margins_name='Total').fillna(value=\" \")#.reset_index()\n",
        "  reporting[f'{name}_{row[0]}'] = df1\n",
        "\n",
        "# lighthousekeeping\n",
        "del c0,c1,df0,df1,row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5IJjzBDwy0nb",
      "metadata": {
        "cellView": "form",
        "id": "5IJjzBDwy0nb"
      },
      "outputs": [],
      "source": [
        "from tables import index  #? necessary\n",
        "#@title rpt_per_diem\n",
        "# %%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_per_diem'\n",
        "# gm_rpt('weekly pd threashold fld_days > 0','doc')\n",
        "# gm_rpt('tot_hrs, exclude vacation (unpaid)')\n",
        "# gm_rpt('C - highlight new employee')\n",
        "# gm_rpt('C - gradient highlight class, type')\n",
        "# gm_rpt('C - compare col.work_at in col.notes != hightlight')\n",
        "\n",
        "df0 = data['pds']\n",
        "df0 = df0.loc[df0.enabled == 'Y'].reset_index(drop=True) # active only\n",
        "c0 = _dict.get('admin_df')\n",
        "c0 = c0.loc[c0.type_.str.contains(name, regex= True, na=False)].reset_index(drop=True)\n",
        "\n",
        "# find new employees, not sure works???\n",
        "reporting['rpt_need_rate'] = df0.loc[df0.pd_calc == '']\n",
        "# reporting[name] = pd.concat([df1, c1]).drop_duplicates(keep=False)\n",
        "\n",
        "# make subs to pds_in\n",
        "reporting['pd_sch'] = c0.loc[c0.term.str.contains('pd_rate', regex= True, na=False),['translate','extra']].reset_index(drop=True)\n",
        "for row in (reporting['pd_sch']).itertuples(index=False):\n",
        "  df0.pd_rate = np.where(df0.pd_rate == row[0], row[1], df0.pd_rate)\n",
        "\n",
        "# get hours and summarize class make sure field is accurate\n",
        "df1 = data['hours']\n",
        "c1 = np.round(pd.pivot_table(df1, values='hours',index=['unm_staff'],columns=['class'],aggfunc=np.sum,\n",
        "                                fill_value=0,margins=True, margins_name='tot_hrs'),2)\n",
        "c1 = c1.rename_axis(columns=None).reset_index()\n",
        "df0 = df0.merge(c1, how='left',left_on='unm_staff', right_on='unm_staff',suffixes=('', '_delete')) # roll up hours into final df, col later\n",
        "\n",
        "# get hours and summarize fld days and hours\n",
        "c1 = df1.loc[df1['class'].str.contains('Field', regex= True, na=False)].reset_index(drop=True) # take out non work items Vaca, etc...\n",
        "c1 = c1.loc[c1.job.str.contains('^\\d{4}', regex= True, na=False)].reset_index(drop=True) # take out non work items Vaca, etc...\n",
        "\n",
        "c2 = split_to_list('unm_staff,dte_loc,hours')\n",
        "c1 = c1.groupby(c2[0:-1]).agg({c2[-1]: ['sum']}).reset_index()\n",
        "c1.columns = c2\n",
        "c1 = pd.pivot_table(c1,index='unm_staff',values='hours',aggfunc=['count',np.sum]).reset_index()\n",
        "c1.columns = split_to_list('uid,fld_days,fld_hours')\n",
        "df0 = df0.merge(c1, how='left',left_on='unm_staff', right_on='uid',suffixes=('', '_delete')) # roll up hours into final df, col later\n",
        "# gm_break()\n",
        "# calculate pd_amt\n",
        "df0['pd_amt'] = 0\n",
        "df0['work_at'] = 'new'\n",
        "df0 = df0[split_to_list('super,nm,group,pd_rate,pd_calc,fld_days,fld_hours,tot_hrs,pd_amt,notes,work_at')].fillna(0)\n",
        "df0 = conv_type(df0,split_to_list('pd_rate,fld_days'),'int')\n",
        "df0['pd_amt'] = np.where(df0.pd_calc == 'daily',df0.pd_rate * df0.fld_days,df0.pd_amt)\n",
        "df0['pd_amt'] = np.where(df0.pd_calc == 'stipend',df0.pd_rate,df0.pd_amt)\n",
        "df0['pd_amt'] = np.where((df0.pd_calc == 'weekly')&(df0.fld_days > 0),df0.pd_rate,df0.pd_amt)\n",
        "df1 = df1.drop_duplicates(subset=['nm', 'work_at']) #.set_index('nm')\n",
        "df0['work_at'] = df0['nm'].map(df1.set_index('nm')['work_at'])\n",
        "\n",
        "# hate this but how lisa needs it\n",
        "fn = lambda x: pd.Series([i for i in reversed(x.split(','))])\n",
        "df0[['fn','ln']] = df0['nm'].apply(fn)\n",
        "df0['nm'] = df0.fn + ' ' + df0.ln\n",
        "df0 = df0.drop(columns = ['fn','ln'])\n",
        "df0 = df0.sort_values(by=df0.columns.to_list()).reset_index(drop=True)\n",
        "\n",
        "# format the report\n",
        "df0.loc[:, 'pd_amt'] = df0['pd_amt'].map('${:,.0f}'.format)\n",
        "df0.loc[:, 'pd_rate'] = df0['pd_rate'].map('${:,.0f}'.format)\n",
        "df0.loc[:, 'fld_days'] = df0['fld_days'].map('{:,.0f}'.format)\n",
        "\n",
        "# gm_rpt('rpt_per_diem_by_super, group shop, office and mech')\n",
        "\n",
        "# print reports\n",
        "reporting[f'{name}_by_super'] = df0.sort_values(by=['super','nm']).set_index(split_to_list('super,nm'))  #.reset_index(drop=True)\n",
        "reporting[name] = df0.drop(columns='super').sort_values(by='nm').reset_index(drop=True)\n",
        "\n",
        "# lighthousekeeping\n",
        "del c0,c1,c2,df0,df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "lQpbS44fpVJa",
      "metadata": {
        "cellView": "form",
        "id": "lQpbS44fpVJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dfb0e30-6b9d-45e6-d347-07f837210973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ],
      "source": [
        "#@title rpt_lisa\n",
        "%%script echo skipping\n",
        "# _dict['debug'] = 'y'\n",
        "\n",
        "name = 'rpt_lisa'\n",
        "\n",
        "df0 = data['hours']\n",
        "df0 = df0.drop(columns=['fn'])\n",
        "\n",
        "alpha = lambda x: pd.Series([i for i in reversed(x.split(','))])\n",
        "df0[['fn','ln']] = df0['nm'].apply(alpha)\n",
        "df0['nm'] = df0.fn + ' ' + df0.ln\n",
        "\n",
        "# df1 = df0.sort_values(by=['nm']).reset_index().drop(columns=['fn'])\n",
        "c0 = split_to_list('nm,job,class_,cost_code,hours')\n",
        "df0 = df0.groupby(c0[0:-1]).agg({c0[-1]: ['sum']}).reset_index()\n",
        "df0 = df0.set_axis(c0, axis=1)\n",
        "df0['entry'] = df0.job + ' | ' + df0.class_ + ' | ' + df0.cost_code\n",
        "reporting[name] = df0[['nm','entry','hours']]\n",
        "\n",
        "# lighthousekeeping\n",
        "del c0,df0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hATJyRPl2Y5q",
      "metadata": {
        "id": "hATJyRPl2Y5q"
      },
      "source": [
        "# report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "W5S8TjbrRTU5",
      "metadata": {
        "cellView": "form",
        "id": "W5S8TjbrRTU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de672a1-b17b-4aed-fc4c-5bf68d0bb1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/github/seacoast_tsheet/reporting\n"
          ]
        }
      ],
      "source": [
        "#@title change_dir\n",
        "name = 'change_dir'\n",
        "\n",
        "c0 = _dict.get('dir_out')\n",
        "%cd $c0\n",
        "\n",
        "# prepare punch_list for printing\n",
        "# c0 = reporting['punch_list']\n",
        "# c0 = split_to_list(c0,'','|')\n",
        "# c0 = pd.DataFrame(c0)\n",
        "\n",
        "# reporting['punch_list'] = c0\n",
        "\n",
        "# reporting['location'] = report_root\n",
        "# _dict['file_name'] = 'v2-QB_hours_' + ''.join(_dict.get('period')) + '.html'\n",
        "c0 = _dict.get('file_name')\n",
        "\n",
        "# close the db connection\n",
        "# db_sql.close_connection()\n",
        "\n",
        "# lighthousekeeping\n",
        "del c0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b8376e7c-e723-472b-8843-9cbdd9c2c490",
      "metadata": {
        "cellView": "form",
        "id": "b8376e7c-e723-472b-8843-9cbdd9c2c490"
      },
      "outputs": [],
      "source": [
        "#@title html report (esparato)\n",
        "# %%script echo skipping\n",
        "\n",
        "c0 = 'Quick Books Time Report<br><img src=\"Logo Seacoast Inc_thumb.png\">'\n",
        "my_page = es.Page(title=c0)\n",
        "es.options.esparto_css = 'esparto.css'\n",
        "\n",
        "intro = \"\"\"\n",
        "##Ensure weekly hours are complete and correct before import to QB\n",
        "\"\"\"\n",
        "\n",
        "intro += \"\"\"###Environment\"\"\"\n",
        "credits = \"\"\"\\\n",
        "<small><i>Text retrieved from [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set) on 2021-04-05</i></small>\n",
        "\"\"\"\n",
        "\n",
        "my_page['introduction'] = 'fix later'  #_dict.get('file_name')\n",
        "my_page.introduction += intro\n",
        "my_page.introduction += pd.DataFrame(_dict.get('file_in'))\n",
        "my_page.introduction += credits\n",
        "my_page['Analysis'] = ''\n",
        "\n",
        "# print report sections\n",
        "c0 = pd.DataFrame(reporting.keys())\n",
        "for row in c0.itertuples(index=False):\n",
        "  my_page['Analysis'][row[0]] = reporting[row[0]]\n",
        "\n",
        "# build_report(reporting)\n",
        "c0 = _dict.get('file_name')\n",
        "my_page.save_html(c0)\n",
        "# my_page.save_pdf(listy)\n",
        "\n",
        "# lighthousekeeping\n",
        "del c0,credits,intro,row\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# email_communications"
      ],
      "metadata": {
        "id": "hkXGnC-KHGj-"
      },
      "id": "hkXGnC-KHGj-"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title email_hours\n",
        "# %%script echo skipping\n",
        "# _dict['send_emails'] = 'y'\n",
        "\n",
        "_dict['send_emails'] = input('step through (y): ')"
      ],
      "metadata": {
        "id": "nxmQs7YzUS_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "9408024b-82c8-40b8-cb94-382adadc983a"
      },
      "id": "nxmQs7YzUS_b",
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step through (y): y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "_QwuSsl6PclR",
      "metadata": {
        "id": "_QwuSsl6PclR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "54aec1e3-865f-46d4-ea82-710cce61b3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "continue (y/n)n\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-704e6b469103>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'send_emails'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mgm_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0;31m# input_ = sorted(d0_['super'].unique()) d0_.info()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mgm_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Clay'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9c1efbfab3ac>\u001b[0m in \u001b[0;36mgm_break\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;34m\"\"\"set breakpoints for debugging\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# def gm_punch(input_,loc_= 'tbd'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "#@title email_hours\n",
        "# %%script echo skipping\n",
        "# _dict['send_emails'] = 'y'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "name = 'email_hours'\n",
        "# gm_rpt('highligh count > 1, super > 1')\n",
        "\n",
        "d0_ = data['hours']\n",
        "\n",
        "# Normal reporting total hours\n",
        "c0 = split_to_list(\"class_,job,super,nm,dte_loc,hours\")\n",
        "df1 = d0_[split_to_list('nm,job')]\n",
        "\n",
        "df1 = df1.astype(str).groupby('nm').agg(lambda x: ' | '.join(x.unique()))\n",
        "d0_['work_at'] = d0_['nm'].map(df1['job'])\n",
        "\n",
        "if _dict.get('send_emails') == 'y':\n",
        "  gm_break()\n",
        "  # input_ = sorted(d0_['super'].unique()) d0_.info()\n",
        "  gm_name = 'Clay'\n",
        "  d1_ = d0_\n",
        "  d1_ = d1_.loc[d1_['super'].str.contains(gm_name)]\n",
        "  # gm_name = '2306'\n",
        "  # d1_ = d1_.loc[d1_.job.str.contains(gm_name)]\n",
        "  c1 = split_to_list(\"class,super,nm,work_at,dte_loc,hours\") # crew chief\n",
        "  # c1 = split_to_list(\"class,nm,work_at,dte_loc,hours\") # single contributor\n",
        "  d1_ = d1_.groupby(c1[0:-1]).agg({c1[-1]: ['sum']}).reset_index()\n",
        "  d1_ = d1_.set_axis(c1, axis=1)  # refactor else where\n",
        "  d1_ = d1_.pivot_table(index=c1[1:-2], columns=c1[-2], values=c1[-1], aggfunc='sum', margins=True, margins_name='Total').fillna(value=\" \")#.reset_index()\n",
        "  d1_\n",
        "\n",
        "# housekeeping\n",
        "del c0,d0_,df1,input_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-tcZBR9INlZt",
      "metadata": {
        "id": "-tcZBR9INlZt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title email_nik_hours\n",
        "# %%script echo skipping\n",
        "# _dict['send_emails'] = 'y'\n",
        "\n",
        "def var(x): return x\n",
        "def df(x): return x\n",
        "\n",
        "name = 'email_nik_hours'\n",
        "\n",
        "d0_ = data['hours']\n",
        "df._pds = data['pds']\n",
        "\n",
        "# input_ = input('nik_hours (y): ') #####################################\n",
        "\n",
        "if _dict.get('send_emails') == 'y':\n",
        "  gm_break()\n",
        "\n",
        "  # painter report\n",
        "  paint_staff = 'painter'\n",
        "  paint_staff = df._pds.loc[df._pds.trade.str.contains(paint_staff),'nm'].to_list()\n",
        "  d1_ = d0_.loc[d0_['nm'].isin(paint_staff)]\n",
        "  c1 = split_to_list(\"class,super,nm,work_at,dte_loc,hours\") # crew chief\n",
        "  d1_ = d1_.groupby(c1[0:-1]).agg({c1[-1]: ['sum']}).reset_index()\n",
        "  d1_ = d1_.set_axis(c1, axis=1)  # refactor else where\n",
        "\n",
        "  input_ = input('report less than 40 only? ')#####################################\n",
        "  if input == 'y':\n",
        "\n",
        "    # Calculate total hours per nm\n",
        "    total_hours_per_nm = d1_.groupby('nm')['hours'].sum().reset_index()\n",
        "\n",
        "    # Filter out nm with total hours less than 40\n",
        "    nm_less_than_40 = total_hours_per_nm.loc[total_hours_per_nm['hours'] < 40, 'nm']\n",
        "\n",
        "    # Filter d1_ based on nm_less_than_40\n",
        "    d1_ = d1_[d1_['nm'].isin(nm_less_than_40)]\n",
        "\n",
        "\n",
        "  d1_ = d1_.pivot_table(index=c1[1:-2], columns=c1[-2], values=c1[-1], aggfunc='sum', margins=True, margins_name='Total').fillna(value=\" \")#.reset_index()\n",
        "  d1_\n",
        "\n",
        "\n",
        "  # non painter report\n",
        "  del d1_\n",
        "  gm_name = 'Parianos'\n",
        "  d1_ = d0_.loc[d0_['super'].str.contains(gm_name, na=False)]\n",
        "  d1_ = d1_.loc[~d1_['nm'].isin(paint_staff)]\n",
        "  # c1 = split_to_list(\"class,super,nm,work_at,dte_loc,hours\") # crew chief\n",
        "  d1_ = d1_.groupby(c1[0:-1]).agg({c1[-1]: ['sum']}).reset_index()\n",
        "  d1_ = d1_.set_axis(c1, axis=1)  # refactor else where\n",
        "  d1_ = d1_.pivot_table(index=c1[1:-2], columns=c1[-2], values=c1[-1], aggfunc='sum', margins=True, margins_name='Total').fillna(value=\" \")#.reset_index()\n",
        "  d1_\n",
        "\n",
        "\n",
        "# housekeeping\n",
        "# del d0_,df1,input_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AsE_oqfpIAEW",
      "metadata": {
        "id": "AsE_oqfpIAEW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title email_pm_job\n",
        "# %%script echo skipping\n",
        "# _dict['send_emails'] = 'y'\n",
        "\n",
        "name = 'email_pm_job'\n",
        "\n",
        "df0 = data['hours']\n",
        "\n",
        "# input_ = input('pm_report (y): ') #####################################\n",
        "\n",
        "if _dict.get('send_emails') == 'y':\n",
        "  gm_break()\n",
        "  gm_name = 'Field'\n",
        "  d0_ = df0.loc[df0['class'].str.contains(gm_name)]\n",
        "  d0_['job_cc'] = d0_['job'] + '_' + d0_['cost_code']\n",
        "  d1_ = d0_.copy()\n",
        "  d1_['crew'] = d1_['nm'].str.split(',').str[1]\n",
        "  d1_ = d1_.sort_values(by=['crew'])\n",
        "  c1 = d1_.groupby('job_cc')['crew'].apply(lambda x: ', '.join(x.dropna().astype(str).unique())).reset_index()\n",
        "  # c1 = d1_.groupby('job_cc')['crew'].apply(lambda x: ', '.join(x.unique())).reset_index()\n",
        "  d0_ = d0_.merge(c1, on='job_cc', how='left')\n",
        "  c1 = split_to_list(\"job,cost_code,crew,dte_loc,hours\")\n",
        "  d0_ = d0_.groupby(c1[0:-1]).agg({c1[-1]: ['sum']}).reset_index()\n",
        "  d0_ = d0_.set_axis(c1, axis=1)  # refactor else where\n",
        "  d1_ = d0_.pivot_table(index=c1[0:-2], columns=c1[-2], values=c1[-1], aggfunc='sum', margins=True, margins_name='Total').fillna(value=\" \")#.reset_index()\n",
        "  d1_\n",
        "\n",
        "  c0 = split_to_list('2103,2111,2302,2312,2313')\n",
        "  d1_ = d0_.loc[d0_['job'].isin(c0)] # Dan\n",
        "  d2_ = d0_.loc[~d0_['job'].isin(c0)] # All others\n",
        "  #add highlight nm in multiple super\n",
        "  grouped = d0_.groupby(['job', 'cost_code'])['hours'].sum().reset_index()\n",
        "\n",
        "  # Create a separate pie chart for each job\n",
        "  for job, job_data in grouped.groupby('job'):\n",
        "      cost_codes = job_data['cost_code']\n",
        "      total_hours = job_data['hours']\n",
        "\n",
        "      plt.figure(figsize=(6, 6))\n",
        "      plt.pie(total_hours, labels=[f'{cost} ({hours:.2f} hours, {hours/sum(total_hours)*100:.2f}%)' for cost, hours in zip(cost_codes, total_hours)], autopct='', startangle=140)\n",
        "      plt.axis('equal')\n",
        "\n",
        "      # Add a title based on the 'job'\n",
        "      plt.title(f\"Total Hours Worked for Job: {job}\")\n",
        "\n",
        "      # Show the pie chart\n",
        "      plt.show()\n",
        "\n",
        "  del d0_,d1_,c0,c1,job,job_data,grouped,cost_codes,total_hours\n",
        "\n",
        "# housekeeping\n",
        "del df0,input_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scratch_Section"
      ],
      "metadata": {
        "id": "vV0e0QywID37"
      },
      "id": "vV0e0QywID37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7-xdmJpokA3c",
      "metadata": {
        "cellView": "form",
        "id": "7-xdmJpokA3c"
      },
      "outputs": [],
      "source": [
        "#@title upgrades_highlighting\n",
        "%%script echo skipping\n",
        "\n",
        "# https://pub.towardsai.net/improve-your-analytical-report-with-conditional-formatting-in-pandas-76039fd3ccb5\n",
        "\n",
        "c0.info()\n",
        "c1.info()\n",
        "c2.info()\n",
        "df0.info()\n",
        "df1.info()\n",
        "list_enum(row)\n",
        "list_enum(_dict.keys())\n",
        "_dict['processed']\n",
        "list_enum(data.keys())\n",
        "data['hours']\n",
        "list_enum(reporting.keys())\n",
        "reporting['rpt_multiple_super']\n",
        "\n",
        "list_enum(split_to_list(reporting.get('punch_list'),'','|'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eosdADeRLELb",
      "metadata": {
        "cellView": "form",
        "id": "eosdADeRLELb"
      },
      "outputs": [],
      "source": [
        "#@title qb_api\n",
        "%%script echo skipping\n",
        "\n",
        "import requests\n",
        "\n",
        "# Set your QuickBooks Time API credentials\n",
        "client_id = \"e651019aa792d0a5a942e6b1f7b444da\"\n",
        "client_secret = \"cd8f6a56eadeedd2fcdf8eb20b2c7ca9\"\n",
        "\n",
        "# Create a session object\n",
        "session = requests.Session()\n",
        "\n",
        "# Set the authorization header\n",
        "session.headers[\"Authorization\"] = \"Bearer 3__5dcc4724df86aef02273429c04a72da326645aab\"\n",
        "session.headers[\"Authorization\"] = \"Bearer {3__5dcc4724df86aef02273429c04a72da326645aab}\".format(client_secret)\n",
        "\n",
        "# Get the list of projects\n",
        "response = session.get(\"https://rest.tsheets.com/api/v1/current_user\")\n",
        "\n",
        "# Check the response status code\n",
        "if response.status_code == 200:\n",
        "    # The request was successful\n",
        "    projects = response.json()\n",
        "\n",
        "    # Print the list of projects\n",
        "    for project in projects:\n",
        "        print(project[\"Name\"])\n",
        "\n",
        "else:\n",
        "    # The request failed\n",
        "    print(response.status_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w7Zxo_AetP_r",
      "metadata": {
        "id": "w7Zxo_AetP_r",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title scratch_py\n",
        "%%script echo skipping\n",
        "# db items_________________________________________________\n",
        "df = data['']\n",
        "table_ = ''\n",
        "db_ = f\"/{_dict.get('dir_in')}/{_dict.get('db')}\"\n",
        "db_sql.delete_table(table_):\n",
        "db_sql = DataSQL(db)\n",
        "db_sql.close_connection()\n",
        "db_sql.check_table_exists(table_)\n",
        "db_sql.load_dataframe(table_)\n",
        "db_sql.save_dataframe(df,table_)\n",
        "# gsheet items____________________________________________________\n",
        "ws = ''\n",
        "data =\n",
        "fn_ = f\"admin_{_dict.get('proj')}\"\n",
        "ws = gc.open(fn_).worksheet(ws)\n",
        "ws.clear()\n",
        "set_with_dataframe(ws, data)\n",
        "# local variables______________________________________________\n",
        "list_enum(row)\n",
        "row_dict\n",
        "list_enum(_dict.keys())\n",
        "list_enum(data.keys())\n",
        "list_enum(reporting.keys())\n",
        "# debugging________________________________________________\n",
        "print(\"ASCII representation of columns in df.gsheet:\", [ord(char) for char in df.gsheet.columns[0]]) #check\n",
        "df.gsheet.columns = df.gsheet.columns.str.replace('[^a-zA-Z0-9_]', '', regex=True) #fix\n",
        "# working area______________________________________________\n",
        "\n",
        "d0 = df.target\n",
        "d1 = df.target\n",
        "d0.info()\n",
        "d1.info()"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "colab": {
      "collapsed_sections": [
        "0nC4jbfODlia",
        "hji9N2p0pcsX",
        "G9lH3GlzfczQ",
        "hATJyRPl2Y5q"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}